//
//  VideoGenerator.swift
//  iEditor
//
//  Created by Atik on 9/10/22.
//

import UIKit
import AVFoundation

public class VideoGenerator: NSObject {
    static var current = VideoGenerator()
    
    private override init() {
        
    }
    
    typealias Completion = (URL?, Error?) -> Void
    
    let setVideoQualityForExporter = AVAssetExportPresetMediumQuality
    let defaultSize = CGSize(width: 1920, height: 1080) // Default video size
    
    /// public property to set a multiple type video's background color
    public static var videoBackgroundColor: UIColor = UIColor.black
    
    /// public property to indicate if the images fed into the generator should be resized to appropriate video ratio 1200 x 1920
    public static var shouldOptimiseImageForVideo: Bool = true
    
    /// public property to set the maximum length of a video
    public static var maxVideoLengthInSeconds: Double? = 3.00
    
    /// public property to set a width to which to resize the images for multiple video generation. Default value is 800
    public static var videoImageWidthForMultipleVideoGeneration = 800
    
    func generate(
        withImages _images: [UIImage],
        _ progress: @escaping ((Progress) -> Void),
        outcome: @escaping (Result<URL, Error>) -> Void
    ) {
        let dispatchQueueGenerate = DispatchQueue(label: "generate", qos: .background)
        
        dispatchQueueGenerate.async { [weak self] in
            
            VideoGenerator.current.setup(withImages: _images)
            /// define the input and output size of the video which will be generated by taking the first image's size
            if let firstImage = VideoGenerator.current.images.first {
                VideoGenerator.current.minSize = firstImage.size
            }
            
            let inputSize = VideoGenerator.current.minSize
            let outputSize = VideoGenerator.current.minSize
            
            self?.getTempVideoFileUrl { (videoOutputURL) in
                do {
                    /// try to create an asset writer for videos pointing to the video url
                    try VideoGenerator.current.videoWriter = AVAssetWriter(outputURL: videoOutputURL, fileType: AVFileType.mp4)
                } catch {
                    VideoGenerator.current.videoWriter = nil
                    DispatchQueue.main.async {
                        outcome(.failure(error))
                    }
                }
                
                /// check if the writer is instantiated successfully
                if let videoWriter = VideoGenerator.current.videoWriter {
                    
                    /// create the basic video settings
                    let videoSettings: [String : AnyObject] = [
                        AVVideoCodecKey  : AVVideoCodecType.h264 as AnyObject,
                        AVVideoWidthKey  : outputSize.width as AnyObject,
                        AVVideoHeightKey : outputSize.height as AnyObject,
                    ]
                    
                    /// create a video writter input
                    let videoWriterInput = AVAssetWriterInput(mediaType: AVMediaType.video, outputSettings: videoSettings)
                    
                    /// create setting for the pixel buffer
                    let sourceBufferAttributes: [String : AnyObject] = [
                        (kCVPixelBufferPixelFormatTypeKey as String): Int(kCVPixelFormatType_32ARGB) as AnyObject,
                        (kCVPixelBufferWidthKey as String): Float(inputSize.width) as AnyObject,
                        (kCVPixelBufferHeightKey as String):  Float(inputSize.height) as AnyObject,
                        (kCVPixelBufferCGImageCompatibilityKey as String): NSNumber(value: true),
                        (kCVPixelBufferCGBitmapContextCompatibilityKey as String): NSNumber(value: true)
                    ]
                    
                    /// create pixel buffer for the input writter and the pixel buffer settings
                    let pixelBufferAdaptor = AVAssetWriterInputPixelBufferAdaptor(assetWriterInput: videoWriterInput, sourcePixelBufferAttributes: sourceBufferAttributes)
                    
                    /// check if an input can be added to the asset
                    assert(videoWriter.canAdd(videoWriterInput))
                    
                    /// add the input writter to the video asset
                    videoWriter.add(videoWriterInput)
                    
                    /// check if a write session can be executed
                    if videoWriter.startWriting() {
                        
                        /// if it is possible set the start time of the session (current at the begining)
                        videoWriter.startSession(atSourceTime: CMTime.zero)
                        
                        /// check that the pixel buffer pool has been created
                        assert(pixelBufferAdaptor.pixelBufferPool != nil)
                        
                        /// create/access separate queue for the generation process
                        let media_queue = DispatchQueue(label: "mediaInputQueue", attributes: [])
                        
                        /// start video generation on a separate queue
                        videoWriterInput.requestMediaDataWhenReady(on: media_queue, using: { () -> Void in
                            
                            /// set up preliminary properties for the image count, frame count and the video elapsed time
                            let numImages = VideoGenerator.current.images.count
                            var frameCount = 0
                            var elapsedTime: Double = 0
                            
                            let currentProgress = Progress(totalUnitCount: Int64(VideoGenerator.current.images.count))
                            
                            var nextStartTimeForFrame: CMTime! = CMTime(seconds: 0, preferredTimescale: 1)
                            var imageForVideo: UIImage!
                            
                            /// if the input writer is ready and we have not yet used all imaged
                            while (videoWriterInput.isReadyForMoreMediaData && frameCount < numImages) {
                                /// get the right photo from the array
                                imageForVideo = VideoGenerator.current.images[frameCount]
                                
                                nextStartTimeForFrame = frameCount == 0 ? CMTime(seconds: 0, preferredTimescale: 600) : CMTime(seconds: Double(elapsedTime), preferredTimescale: 600)
                                
                                let audio_Time = 3.00 //VideoGenerator.current.audioDurations[0]
                                let total_Images = VideoGenerator.current.images.count
                                elapsedTime += audio_Time / Double(total_Images)
                                
                                /// append the image to the pixel buffer at the right start time
                                if !VideoGenerator.current.appendPixelBufferForImage(imageForVideo, pixelBufferAdaptor: pixelBufferAdaptor, presentationTime: nextStartTimeForFrame) {
                                    DispatchQueue.main.async {
                                        VideoGenerator.current.videoWriter = nil
                                        outcome(.failure(VideoGeneratorError(error: .kFailedToAppendPixelBufferError)))
                                    }
                                }
                                
                                // increise the frame count
                                frameCount += 1
                                
                                currentProgress.completedUnitCount = Int64(frameCount)
                                
                                // after each successful append of an image track the current progress
                                progress(currentProgress)
                            }
                            
                            // after all images are appended the writting shoul be marked as finished
                            videoWriterInput.markAsFinished()
                            
                            if let _maxLength = VideoGenerator.maxVideoLengthInSeconds {
                                videoWriter.endSession(atSourceTime: CMTime(seconds: _maxLength, preferredTimescale: 1))
                            }
                            
                            // the completion is made with a completion handler which will return the url of the generated video or an error
                            videoWriter.finishWriting { () -> Void in
                                if let documentsPath = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true).first {
                                    let newPath = URL(fileURLWithPath: documentsPath).appendingPathComponent("MergedPhotos.m4v")
                                    self?.deleteFile(pathURL: newPath, completion: {
                                        try FileManager.default.moveItem(at: videoOutputURL, to: newPath)
                                    })
                                    print("Merged Video Saved to: \(newPath)")
                                    DispatchQueue.main.async {
                                        outcome(.success(newPath))
                                    }
                                }
                                
                                VideoGenerator.current.videoWriter = nil
                            }
                        })
                    } else {
                        DispatchQueue.main.async {
                            VideoGenerator.current.videoWriter = nil
                            outcome(.failure(VideoGeneratorError(error: .kFailedToStartAssetWriterError)))
                        }
                    }
                } else {
                    DispatchQueue.main.async {
                        VideoGenerator.current.videoWriter = nil
                        outcome(.failure(VideoGeneratorError(error: .kFailedToStartAssetWriterError)))
                    }
                }
            }
        }
    }
    
    fileprivate func setup(withImages _images: [UIImage]) {
        images = []
        var datasImages = [Data?]()
        
        /// guard against missing images or audio
        guard !_images.isEmpty else {
            return
        }
        
        for _image in _images {
            autoreleasepool {
                if let imageData = _image.scaleImageToSize(newSize: CGSize(width: VideoGenerator.videoImageWidthForMultipleVideoGeneration, height: VideoGenerator.videoImageWidthForMultipleVideoGeneration))?.pngData() {
                    datasImages.append(imageData)
                }
            }
        }
        
        datasImages.forEach {
            if let imageData = $0, let image = UIImage(data: imageData, scale: UIScreen.main.scale) {
                self.images.append(image)
            }
        }
        
        datasImages.removeAll()
        
        let minVideoDuration = Double(CMTime(seconds: minSingleVideoDuration, preferredTimescale: 1).seconds)
        duration = minVideoDuration
    }
    
    /// private property to store the images from which a video will be generated
    fileprivate var images: [UIImage] = []
    
    /// private property to store the duration of the generated video
    fileprivate var duration: Double! = 1.0
    
    /// private property to store a video asset writer (optional because the generation might fail)
    fileprivate var videoWriter: AVAssetWriter? {
        didSet {
            if videoWriter == nil {
                images.removeAll()
            }
        }
    }
    
    /// private property to store the minimum size for the video
    fileprivate var minSize = CGSize.zero
    
    /// private property to store the minimum duration for a single video
    fileprivate var minSingleVideoDuration: Double = 3.0
    
    /// private property to store the video resource for reversing
    fileprivate var reversedVideoURL: URL?
    
    private func appendPixelBufferForImage(_ image: UIImage, pixelBufferAdaptor: AVAssetWriterInputPixelBufferAdaptor, presentationTime: CMTime) -> Bool {
        
        /// at the beginning of the append the status is false
        var appendSucceeded = false
        
        /**
         *  The proccess of appending new pixels is put inside a autoreleasepool
         */
        autoreleasepool {
            
            // check posibilitty of creating a pixel buffer pool
            if let pixelBufferPool = pixelBufferAdaptor.pixelBufferPool {
                
                let pixelBufferPointer = UnsafeMutablePointer<CVPixelBuffer?>.allocate(capacity: MemoryLayout<CVPixelBuffer?>.size)
                let status: CVReturn = CVPixelBufferPoolCreatePixelBuffer(
                    kCFAllocatorDefault,
                    pixelBufferPool,
                    pixelBufferPointer
                )
                
                /// check if the memory of the pixel buffer pointer can be accessed and the creation status is 0
                if let pixelBuffer = pixelBufferPointer.pointee, status == 0 {
                    
                    // if the condition is satisfied append the image pixels to the pixel buffer pool
                    fillPixelBufferFromImage(image, pixelBuffer: pixelBuffer)
                    
                    // generate new append status
                    appendSucceeded = pixelBufferAdaptor.append(
                        pixelBuffer,
                        withPresentationTime: presentationTime
                    )
                    
                    /**
                     *  Destroy the pixel buffer contains
                     */
                    pixelBufferPointer.deinitialize(count: 1)
                } else {
                    NSLog("error: Failed to allocate pixel buffer from pool")
                }
                
                /**
                 Destroy the pixel buffer pointer from the memory
                 */
                pixelBufferPointer.deallocate()
            }
        }
        
        return appendSucceeded
    }
    
    /**
     Private method to append image pixels to a pixel buffer
     
     - parameter image:       The image which pixels will be appented
     - parameter pixelBuffer: The pixel buffer (as memory) to which the image pixels will be appended
     */
    private func fillPixelBufferFromImage(_ image: UIImage, pixelBuffer: CVPixelBuffer) {
        // lock the buffer memoty so no one can access it during manipulation
        CVPixelBufferLockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: CVOptionFlags(0)))
        
        // get the pixel data from the address in the memory
        let pixelData = CVPixelBufferGetBaseAddress(pixelBuffer)
        
        // create a color scheme
        let rgbColorSpace = CGColorSpaceCreateDeviceRGB()
        
        /// set the context size
        let contextSize = image.size
        
        // generate a context where the image will be drawn
        if let context = CGContext(data: pixelData, width: Int(contextSize.width), height: Int(contextSize.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue) {
            
            var imageHeight = image.size.height
            var imageWidth = image.size.width
            
            if Int(imageHeight) > context.height {
                imageHeight = 16 * (CGFloat(context.height) / 16).rounded(.awayFromZero)
            } else if Int(imageWidth) > context.width {
                imageWidth = 16 * (CGFloat(context.width) / 16).rounded(.awayFromZero)
            }
            
            let center = CGPoint(x: (minSize.width - imageWidth) / 2, y: (minSize.height - imageHeight) / 2)
            
            context.clear(CGRect(x: 0.0, y: 0.0, width: imageWidth, height: imageHeight))
            
            // set the context's background color
            context.setFillColor(VideoGenerator.videoBackgroundColor.cgColor)
            context.fill(CGRect(x: 0.0, y: 0.0, width: CGFloat(context.width), height: CGFloat(context.height)))
            
            context.concatenate(.identity)
            
            // draw the image in the context
            
            if let cgImage = image.cgImage {
                context.draw(cgImage, in: CGRect(x: center.x, y: center.y, width: imageWidth, height: imageHeight))
            }
            
            // unlock the buffer memory
            CVPixelBufferUnlockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: CVOptionFlags(0)))
        }
    }
    
    /// Private method to delete the temp video file
    ///
    /// - Returns: the temp file url
    private func getTempVideoFileUrl(completion: @escaping (URL) -> ()) {
        DispatchQueue.main.async {
            if let documentsPath = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true).first {
                let testOutputURL = URL(fileURLWithPath: documentsPath).appendingPathComponent("test.m4v")
                do {
                    if FileManager.default.fileExists(atPath: testOutputURL.path) {
                        try FileManager.default.removeItem(at: testOutputURL)
                    }
                } catch {
                    print(error.localizedDescription)
                }
                
                completion(testOutputURL)
            }
        }
    }
    
    /// Private method to delete a file
    ///
    /// - Parameters:
    ///   - pathURL: the file's path
    ///   - completion: a blick to handle completion
    private func deleteFile(pathURL: URL, completion: @escaping () throws -> ()) {
        DispatchQueue.main.async {
            do {
                if FileManager.default.fileExists(atPath: pathURL.path) {
                    try FileManager.default.removeItem(at: pathURL)
                }
            } catch {
                print(error.localizedDescription)
            }
            
            do {
                try completion()
            } catch {
                print(error.localizedDescription)
            }
        }
    }
}

extension VideoGenerator {
    func addTextWithFrame(videoURL: URL, name: String, onComplete: @escaping (URL?) -> Void) {
        let asset = AVURLAsset(url: videoURL)
        let composition = AVMutableComposition()
        
        guard
            let compositionTrack = composition.addMutableTrack(
                withMediaType: .video, preferredTrackID: kCMPersistentTrackID_Invalid),
            let assetTrack = asset.tracks(withMediaType: .video).first
        else {
            print("Something is wrong with the asset.")
            onComplete(nil)
            return
        }
        
        do {
            let timeRange = CMTimeRange(start: .zero, duration: asset.duration)
            try compositionTrack.insertTimeRange(timeRange, of: assetTrack, at: .zero)
            
            if let audioAssetTrack = asset.tracks(withMediaType: .audio).first,
               let compositionAudioTrack = composition.addMutableTrack(
                withMediaType: .audio,
                preferredTrackID: kCMPersistentTrackID_Invalid) {
                try compositionAudioTrack.insertTimeRange(
                    timeRange,
                    of: audioAssetTrack,
                    at: .zero)
            }
        } catch {
            print(error)
            onComplete(nil)
            return
        }
        
        compositionTrack.preferredTransform = assetTrack.preferredTransform
        let videoInfo = orientation(from: assetTrack.preferredTransform)
        
        let videoSize: CGSize
        if videoInfo.isPortrait {
            videoSize = CGSize(
                width: assetTrack.naturalSize.height,
                height: assetTrack.naturalSize.width)
        } else {
            videoSize = assetTrack.naturalSize
        }
        
        let backgroundLayer = CALayer()
        backgroundLayer.frame = CGRect(origin: .zero, size: videoSize)
        let videoLayer = CALayer()
        videoLayer.frame = CGRect(origin: .zero, size: videoSize)
        let overlayLayer = CALayer()
        overlayLayer.frame = CGRect(origin: .zero, size: videoSize)
        
        backgroundLayer.backgroundColor = UIColor.systemMint.cgColor
        videoLayer.frame = CGRect(
            x: 10,
            y: 10,
            width: videoSize.width - 20,
            height: videoSize.height - 20
        )
        
        backgroundLayer.contents = UIImage(named: "background")?.cgImage
        backgroundLayer.contentsGravity = .resizeAspectFill
        
        add(
            text: name,
            to: overlayLayer,
            videoSize: videoSize
        )
        
        let outputLayer = CALayer()
        outputLayer.frame = CGRect(origin: .zero, size: videoSize)
        outputLayer.addSublayer(backgroundLayer)
        outputLayer.addSublayer(videoLayer)
        outputLayer.addSublayer(overlayLayer)
        
        let videoComposition = AVMutableVideoComposition()
        videoComposition.renderSize = videoSize
        videoComposition.frameDuration = CMTime(value: 1, timescale: asset.duration.timescale)
        videoComposition.animationTool = AVVideoCompositionCoreAnimationTool(
            postProcessingAsVideoLayer: videoLayer,
            in: outputLayer)
        
        let instruction = AVMutableVideoCompositionInstruction()
        instruction.timeRange = CMTimeRange(
            start: .zero,
            duration: composition.duration)
        
        let layerInstruction = compositionLayerInstruction(
            for: compositionTrack,
            assetTrack: assetTrack)
        instruction.layerInstructions = [layerInstruction]
        
        videoComposition.instructions = [instruction]
        
        guard let export = AVAssetExportSession(
            asset: composition,
            presetName: setVideoQualityForExporter)
        else {
            print("Cannot create export session.")
            onComplete(nil)
            return
        }
        
        let videoName = UUID().uuidString
        let exportURL = URL(fileURLWithPath: NSTemporaryDirectory())
            .appendingPathComponent(videoName)
            .appendingPathExtension(".mp4")
        
        export.videoComposition = videoComposition
        export.outputFileType = .mp4
        export.outputURL = exportURL
        print("Exporting...")
        export.exportAsynchronously {
            DispatchQueue.main.async {
                switch export.status {
                case .completed:
                    onComplete(exportURL)
                default:
                    print("Something went wrong during export.")
                    print(export.error ?? "unknown error")
                    onComplete(nil)
                    break
                }
            }
        }
    }
    
    private func add(text: String, to layer: CALayer, videoSize: CGSize) {
        let attributedText = NSAttributedString(
            string: text,
            attributes: [
                .font: UIFont(name: "ArialRoundedMTBold", size: 40) as Any,
                .foregroundColor: UIColor.blue,
                .strokeColor: UIColor.white,
                .strokeWidth: -3])
        
        let textLayer = CATextLayer()
        textLayer.string = attributedText
        textLayer.shouldRasterize = true
        textLayer.rasterizationScale = UIScreen.main.scale
        textLayer.backgroundColor = UIColor.clear.cgColor
        textLayer.alignmentMode = .center
        
        textLayer.frame = CGRect(
            x: 0,
            y: videoSize.height * 0.66,
            width: videoSize.width,
            height: 150)
        textLayer.displayIfNeeded()
        
        let scaleAnimation = CABasicAnimation(keyPath: "transform.scale")
        scaleAnimation.fromValue = 0.8
        scaleAnimation.toValue = 1.2
        scaleAnimation.duration = 0.5
        scaleAnimation.repeatCount = .greatestFiniteMagnitude
        scaleAnimation.autoreverses = true
        scaleAnimation.timingFunction = CAMediaTimingFunction(name: .easeInEaseOut)
        
        scaleAnimation.beginTime = AVCoreAnimationBeginTimeAtZero
        scaleAnimation.isRemovedOnCompletion = false
        textLayer.add(scaleAnimation, forKey: "scale")
        
        layer.addSublayer(textLayer)
    }
    
    private func orientation(from transform: CGAffineTransform) -> (orientation: UIImage.Orientation, isPortrait: Bool) {
        var assetOrientation = UIImage.Orientation.up
        var isPortrait = false
        if transform.a == 0 && transform.b == 1.0 && transform.c == -1.0 && transform.d == 0 {
            assetOrientation = .right
            isPortrait = true
        } else if transform.a == 0 && transform.b == -1.0 && transform.c == 1.0 && transform.d == 0 {
            assetOrientation = .left
            isPortrait = true
        } else if transform.a == 1.0 && transform.b == 0 && transform.c == 0 && transform.d == 1.0 {
            assetOrientation = .up
        } else if transform.a == -1.0 && transform.b == 0 && transform.c == 0 && transform.d == -1.0 {
            assetOrientation = .down
        }
        
        return (assetOrientation, isPortrait)
    }
    
    private func compositionLayerInstruction(for track: AVCompositionTrack, assetTrack: AVAssetTrack) -> AVMutableVideoCompositionLayerInstruction {
        let instruction = AVMutableVideoCompositionLayerInstruction(assetTrack: track)
        let transform = assetTrack.preferredTransform
        
        instruction.setTransform(transform, at: .zero)
        
        return instruction
    }
}

extension VideoGenerator {
    func mergeWithAnimation(arrayVideos:[AVAsset], completion:@escaping Completion) -> Void {
        doMerge(arrayVideos: arrayVideos, animation: true, completion: completion)
    }
    
    func ApplyFilter(video: AVAsset, filter: String, completion:@escaping Completion) -> Void {
        // Init composition
        let mixComposition = AVMutableComposition.init()
        
        // Get video track
        guard let videoTrack = video.tracks(withMediaType: AVMediaType.video).first else {
            completion(nil, nil)
            return
        }
        
        _ = videoTrack.naturalSize
        let insertTime = CMTime.zero
        
        // Init video & audio composition track
        let videoCompositionTrack = mixComposition.addMutableTrack(withMediaType: AVMediaType.video,
                                                                   preferredTrackID: Int32(kCMPersistentTrackID_Invalid))
        
        let startTime = CMTime.zero
        let duration = video.duration
        
        do {
            // Add video track to video composition at specific time
            try videoCompositionTrack?.insertTimeRange(CMTimeRangeMake(start: startTime, duration: duration),
                                                       of: videoTrack,
                                                       at: insertTime)
        }
        catch {
            print("Load track error \(error)")
        }
        let composition = AVVideoComposition(asset: video, applyingCIFiltersWithHandler: { request in
            let filtered = request.sourceImage.applyingFilter(filter, parameters: [:])
            // Provide the filter output to the composition
            request.finish(with: filtered, context: nil)
        })
        
        let path = NSTemporaryDirectory().appending("AfterFilterVideo.mp4")
        let exportURL = URL.init(fileURLWithPath: path)
        
        // Check exist and remove old file
        FileManager.default.removeItemIfExisted(exportURL)
        
        // Init exporter
        let exporter = AVAssetExportSession.init(asset: mixComposition, presetName: setVideoQualityForExporter)
        exporter?.outputURL = exportURL
        exporter?.outputFileType = AVFileType.mp4
        exporter?.shouldOptimizeForNetworkUse = true
        exporter?.videoComposition = composition
        
        // Do export
        exporter?.exportAsynchronously(completionHandler: {
            DispatchQueue.main.async {
                self.exportDidFinish(exporter: exporter, videoURL: exportURL, completion: completion)
            }
        })
    }
    
    private func doMerge(arrayVideos:[AVAsset], animation:Bool, completion:@escaping Completion) -> Void {
        var insertTime = CMTime.zero
        var arrayLayerInstructions:[AVMutableVideoCompositionLayerInstruction] = []
        var outputSize = CGSize.init(width: 0, height: 0)
        
        // Determine video output size
        for videoAsset in arrayVideos {
            let videoTrack = videoAsset.tracks(withMediaType: AVMediaType.video)[0]
            
            let assetInfo = orientationFromTransform(transform: videoTrack.preferredTransform)
            
            var videoSize = videoTrack.naturalSize
            if assetInfo.isPortrait == true {
                videoSize.width = videoTrack.naturalSize.height
                videoSize.height = videoTrack.naturalSize.width
            }
            
            if videoSize.height > outputSize.height {
                outputSize = videoSize
            }
        }
        
        if outputSize.width == 0 || outputSize.height == 0 {
            outputSize = defaultSize
        }
        
        guard let bgAudioURL = Bundle.main.url(forResource: "mixaund-bright-morning", withExtension: "mp3") else {
            return
        }
        let silenceAsset = AVAsset(url: bgAudioURL)
        let silenceSoundTrack = silenceAsset.tracks(withMediaType: AVMediaType.audio).first
        var audioTime = CMTime.zero
        
        // Init composition
        let mixComposition = AVMutableComposition.init()
        let audioCompositionTrack = mixComposition.addMutableTrack(withMediaType: AVMediaType.audio,
                                                                   preferredTrackID: Int32(kCMPersistentTrackID_Invalid))
        let audioTrack:AVAssetTrack? = silenceSoundTrack
        //print("audioTime: \(silenceAsset.duration.seconds)")
        
        for videoAsset in arrayVideos {
            // Get video track
            guard let videoTrack = videoAsset.tracks(withMediaType: AVMediaType.video).first else { continue }
            let videoCompositionTrack = mixComposition.addMutableTrack(withMediaType: AVMediaType.video,
                                                                       preferredTrackID: Int32(kCMPersistentTrackID_Invalid))
            
            
            do {
                let startTime = CMTime.zero
                let duration = videoAsset.duration
                //print("duration: \(duration.seconds)")
                
                // Add video track to video composition at specific time
                try videoCompositionTrack?.insertTimeRange(CMTimeRangeMake(start: startTime, duration: duration),
                                                           of: videoTrack,
                                                           at: insertTime)
                
                // Add instruction for video track
                let layerInstruction = videoCompositionInstructionForTrack(track: videoCompositionTrack!,
                                                                           asset: videoAsset,
                                                                           standardSize: outputSize,
                                                                           atTime: insertTime)
                
                // Hide video track before changing to new track
                let endTime = CMTimeAdd(insertTime, duration)
                //print("endTime: \(endTime.seconds)")
                
                if animation {
                    let timeScale = videoAsset.duration.timescale
                    let durationAnimation = CMTime.init(seconds: 1, preferredTimescale: timeScale)
                    
                    layerInstruction.setOpacityRamp(fromStartOpacity: 1.0, toEndOpacity: 0.0, timeRange: CMTimeRange.init(start: endTime, duration: durationAnimation))
                }
                else {
                    layerInstruction.setOpacity(0, at: endTime)
                }
                
                arrayLayerInstructions.append(layerInstruction)
                
                // Increase the insert time
                insertTime = CMTimeAdd(insertTime, duration)
                //print("insertTime: \(insertTime.seconds)")
                audioTime = insertTime
            }
            catch {
                debugPrint("Load track error")
            }
        }
        
        if let audioTrack = audioTrack {
            do {
                try audioCompositionTrack?.insertTimeRange(CMTimeRangeMake(start: CMTime.zero, duration: audioTime),
                                                           of: audioTrack,
                                                           at: CMTime.zero)
            }
            catch {
                debugPrint("Load track error")
            }
        }
        
        // Main video composition instruction
        let mainInstruction = AVMutableVideoCompositionInstruction()
        mainInstruction.timeRange = CMTimeRangeMake(start: CMTime.zero, duration: insertTime)
        mainInstruction.layerInstructions = arrayLayerInstructions
        
        // Main video composition
        let mainComposition = AVMutableVideoComposition()
        mainComposition.instructions = [mainInstruction]
        mainComposition.frameDuration = CMTimeMake(value: 1, timescale: 30)
        mainComposition.renderSize = outputSize
        //mainComposition.
        
        // Export to file
        
        var path = "";
        
        if arrayVideos.count == 2 {
            path = NSTemporaryDirectory().appending("mergedVideo.mp4");
        }else{
            let ticks = Date().ticks
            path = NSTemporaryDirectory().appending("\(ticks)mergedVideo.mp4");
        }
        let exportURL = URL.init(fileURLWithPath: path)
        
        // Remove file if existed
        FileManager.default.removeItemIfExisted(exportURL)
        
        // Init exporter
        let exporter = AVAssetExportSession.init(asset: mixComposition, presetName: setVideoQualityForExporter)
        exporter?.outputURL = exportURL
        exporter?.outputFileType = AVFileType.mp4
        exporter?.shouldOptimizeForNetworkUse = true
        exporter?.videoComposition = mainComposition
        
        // Do export
        exporter?.exportAsynchronously(completionHandler: {
            DispatchQueue.main.async {
                self.exportDidFinish(exporter: exporter, videoURL: exportURL, completion: completion)
            }
        })
        
    }
    
    fileprivate func exportDidFinish(exporter:AVAssetExportSession?, videoURL:URL, completion:@escaping Completion) -> Void {
        if exporter?.status == AVAssetExportSession.Status.completed {
            // debugPrint("Exported file: \(videoURL.absoluteString)")
            completion(videoURL,nil)
        }
        else if exporter?.status == AVAssetExportSession.Status.failed {
            completion(videoURL,exporter?.error)
        }
    }
    
    fileprivate func orientationFromTransform(transform: CGAffineTransform) -> (orientation: UIImage.Orientation, isPortrait: Bool) {
        var assetOrientation = UIImage.Orientation.up
        var isPortrait = false
        if transform.a == 0 && transform.b == 1.0 && transform.c == -1.0 && transform.d == 0 {
            assetOrientation = .right
            isPortrait = true
        } else if transform.a == 0 && transform.b == -1.0 && transform.c == 1.0 && transform.d == 0 {
            assetOrientation = .left
            isPortrait = true
        } else if transform.a == 1.0 && transform.b == 0 && transform.c == 0 && transform.d == 1.0 {
            assetOrientation = .up
        } else if transform.a == -1.0 && transform.b == 0 && transform.c == 0 && transform.d == -1.0 {
            assetOrientation = .down
        }
        return (assetOrientation, isPortrait)
    }
    
    fileprivate func videoCompositionInstructionForTrack(track: AVCompositionTrack, asset: AVAsset, standardSize:CGSize, atTime: CMTime) -> AVMutableVideoCompositionLayerInstruction {
        let instruction = AVMutableVideoCompositionLayerInstruction(assetTrack: track)
        let assetTrack = asset.tracks(withMediaType: AVMediaType.video)[0]
        
        let transform = assetTrack.preferredTransform
        let assetInfo = orientationFromTransform(transform: transform)
        
        var aspectFillRatio:CGFloat = 1
        if assetTrack.naturalSize.height < assetTrack.naturalSize.width {
            aspectFillRatio = standardSize.height / assetTrack.naturalSize.height
        }
        else {
            aspectFillRatio = standardSize.width / assetTrack.naturalSize.width
        }
        
        if assetInfo.isPortrait {
            let scaleFactor = CGAffineTransform(scaleX: aspectFillRatio, y: aspectFillRatio)
            
            let posX = standardSize.width/2 - (assetTrack.naturalSize.height * aspectFillRatio)/2
            let posY = standardSize.height/2 - (assetTrack.naturalSize.width * aspectFillRatio)/2
            let moveFactor = CGAffineTransform(translationX: posX, y: posY)
            
            instruction.setTransform(assetTrack.preferredTransform.concatenating(scaleFactor).concatenating(moveFactor), at: atTime)
            
        } else {
            let scaleFactor = CGAffineTransform(scaleX: aspectFillRatio, y: aspectFillRatio)
            
            let posX = standardSize.width/2 - (assetTrack.naturalSize.width * aspectFillRatio)/2
            let posY = standardSize.height/2 - (assetTrack.naturalSize.height * aspectFillRatio)/2
            let moveFactor = CGAffineTransform(translationX: posX, y: posY)
            
            var concat = assetTrack.preferredTransform.concatenating(scaleFactor).concatenating(moveFactor)
            
            if assetInfo.orientation == .down {
                let fixUpsideDown = CGAffineTransform(rotationAngle: CGFloat(Double.pi))
                concat = fixUpsideDown.concatenating(scaleFactor).concatenating(moveFactor)
            }
            
            instruction.setTransform(concat, at: atTime)
        }
        return instruction
    }
}
